# üèóÔ∏è Arquitetura de Dados e RAG - Vox AI

Este documento detalha a arquitetura t√©cnica do **Vox AI**, com foco no fluxo de dados, esquema do banco de dados vetorial e estrat√©gia de recupera√ß√£o de informa√ß√£o (RAG).

## üîÑ Fluxo de Dados (Data Flow)

O diagrama abaixo ilustra o ciclo de vida de uma intera√ß√£o do usu√°rio, desde a entrada do prompt at√© a gera√ß√£o da resposta enriquecida pelo contexto.

```mermaid
graph TD
    A("üë§ Usu√°rio") -->|Prompt| B("üñ•Ô∏è Streamlit Frontend")
    B -->|Texto| C("‚öôÔ∏è Embeddings")
    C -->|API| D("Google Gemini")
    D -->|Vetor| E[("üóÑÔ∏è Supabase DB")]
    
    subgraph Retrieval [Retrieval - Busca Sem√¢ntica]
        E -->|RPC| F("Busca Vetorial")
        F -->|Threshold| G("Filtro de T√≥picos")
        G -->|L√≥gica RAG| H("Recupera√ß√£o Chunks")
    end
    
    H -->|Contexto| I("ü§ñ LLM: Gemini 1.5")
    I -->|Resposta| B
    B -.->|Exibir| A
```

## üóÑÔ∏è Database Schema (Supabase/PostgreSQL)

O Vox utiliza o PostgreSQL com a extens√£o `pgvector` gerenciado pelo Supabase. Abaixo est√£o as principais tabelas e suas fun√ß√µes arquiteturais.

### 1. Knowledge Base (`knowledge_base`)
Armazena os fragmentos de informa√ß√£o curada para o RAG.

| Coluna | Tipo | Descri√ß√£o |
| :--- | :--- | :--- |
| `kb_id` | `text` (PK) | Identificador √∫nico (ex: `vox-kb-0001`). |
| `topico` | `text` | Categoria macro para agrupamento l√≥gico. |
| `descricao` | `text` | O conte√∫do textual real (Chunk) usado no contexto. |
| `embedding` | `vector(768)` | Vetor gerado pelo modelo `text-embedding-004`. |
| `modificado_em` | `timestamp` | Controle de vers√£o do dado. |

> **Nota T√©cnica:** Utilizamos √≠ndices HNSW (`hnsw`) na coluna de embedding para garantir performance em buscas de alta dimensionalidade, sacrificando um pouco de precis√£o por velocidade em escala.

### 2. Logs de Chat (`chat_logs` & `chat_logs_kb`)
Permitem rastreabilidade e auditoria das respostas da IA.

- **`chat_logs`**: Armazena o prompt do usu√°rio, a resposta da IA e a vers√£o do c√≥digo (`git_version`) no momento da resposta.
- **`chat_logs_kb`**: Tabela de jun√ß√£o que registra exatamente quais fragmentos (`kb_id`) foram usados para compor aquela resposta espec√≠fica. Isso √© crucial para debugar alucina√ß√µes.

## üß† Estrat√©gia de RAG (Retrieval-Augmented Generation)

Nossa implementa√ß√£o de RAG foge do b√°sico para garantir maior assertividade.

### 1. Modelo de Embedding
Utilizamos o **Google text-embedding-004**, que gera vetores de 768 dimens√µes. Escolhemos este modelo pelo equil√≠brio entre custo e performance sem√¢ntica em l√≠ngua portuguesa.

### 2. Recupera√ß√£o de Contexto Inteligente
N√£o fazemos apenas uma busca "burra" pelos Top-K resultados. Implementamos uma l√≥gica de densidade de t√≥picos no arquivo `src/core/database.py`:

1.  **Busca Inicial**: Recuperamos os 10 chunks mais similares (Threshold 0.5).
2.  **An√°lise de T√≥pico**: O algoritmo conta a frequ√™ncia dos t√≥picos retornados.
3.  **Expans√£o de Contexto**: Se um t√≥pico aparece mais de 3 vezes na busca inicial (indicando alta relev√¢ncia), o sistema descarta os chunks isolados e busca o contexto completo daquele t√≥pico.
    *   *Objetivo*: Fornecer ao LLM o contexto completo de um assunto (ex: "Protocolo PrEP") em vez de frases soltas.

## 3. Stack Tecnol√≥gica

- **Orquestra√ß√£o**: Python 3.11 + Streamlit
- **Vector Store**: Supabase (`pgvector`)
- **LLM & Embeddings**: Google Gemini API
- **CI/CD**: GitHub Actions (Deploy autom√°tico de Migrations)

<div align="center">
<p>ü§ñ Vox AI: conversas que importam üè≥Ô∏è‚Äçüåà</p>
</div>